{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Please download the dataset from the following link: \n",
    "\n",
    "# https://archive.ics.uci.edu/ml/datasets/Devanagari+Handwritten+Character+Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# You can download the csv files of the training as well as cross validation dataset from the following links:\n",
    "\n",
    "# https://drive.google.com/file/d/1LO7l5LqiGgRg_EUJhrwtrw6TRGeixahc/view?usp=sharing\n",
    "\n",
    "# https://drive.google.com/file/d/16KYVxtexm1Jzw_d7sv9ql3S33571U_J8/view?usp=sharingv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,accuracy_score,precision_score,recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import iglob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class img_to_df:\n",
    "    \n",
    "    def __init__(self,path,train_cv_split):\n",
    "        self.path = path\n",
    "        self.unique_labels = os.listdir(path)\n",
    "        self.train_cv_split = train_cv_split\n",
    "        \n",
    "\n",
    "    def list_of_images(self,folder):\n",
    "        return iglob(os.path.join(self.path,folder)+\"/*.png\")\n",
    "    \n",
    "    \n",
    "    def read_image(self,folder_image):\n",
    "        image = plt.imread(folder_image)\n",
    "        return image.reshape(image.shape[0]*image.shape[1],)\n",
    "    \n",
    "    \n",
    "    def stacking_row_vectors(self,folder):\n",
    "        images_list_gen = self.list_of_images(folder)\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=16) as p:\n",
    "            lazy_loop = p.map(self.read_image,images_list_gen)\n",
    "            \n",
    "        return lazy_loop\n",
    "    \n",
    "    \n",
    "    def generate_df(self):\n",
    "        train_data = list()\n",
    "        cv_data = list()\n",
    "        \n",
    "        for folder in self.unique_labels:\n",
    "            dir_images_gen = self.stacking_row_vectors(folder)\n",
    "            \n",
    "            train_folder_matrix = list()\n",
    "            cv_folder_matrix = list()\n",
    "            \n",
    "            for _ in range(int(self.train_cv_split[0]*1700)):\n",
    "                try:\n",
    "                    train_folder_matrix.append(next(dir_images_gen))\n",
    "                except StopIteration:\n",
    "                    break\n",
    "                \n",
    "            for _ in range(int(self.train_cv_split[1]*1700)):\n",
    "                try:\n",
    "                    cv_folder_matrix.append(next(dir_images_gen))\n",
    "                except StopIteration:\n",
    "                    break\n",
    "            \n",
    "            train_folder_matrix = np.array(train_folder_matrix)\n",
    "            cv_folder_matrix = np.array(cv_folder_matrix)\n",
    "            \n",
    "            train_data.append(train_folder_matrix)\n",
    "            cv_data.append(cv_folder_matrix)\n",
    "            \n",
    "        train_data = np.concatenate(train_data,axis=0)\n",
    "        cv_data = np.concatenate(cv_data,axis=0)\n",
    "        train_labels = list()\n",
    "        cv_labels = list()\n",
    "        \n",
    "        for folder_name in self.unique_labels:\n",
    "            train_labels = train_labels + [folder_name]*train_folder_matrix.shape[0]\n",
    "            cv_labels = cv_labels + [folder_name]*cv_folder_matrix.shape[0]\n",
    "            \n",
    "        train_data = pd.DataFrame(data=train_data)\n",
    "        train_data['label'] = train_labels\n",
    "        cv_data = pd.DataFrame(data=cv_data)\n",
    "        cv_data['label'] = cv_labels\n",
    "        return train_data,cv_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = img_to_df(\"./Train\",(0.8,0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, cv_data = obj.generate_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.to_csv(\"./Devnagari_Handwritten_Character_Train.csv\")\n",
    "\n",
    "cv_data.to_csv(\"./Devnagari_Handwritten_Character_cv.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_csv(\"Devnagari_Handwritten_Character_Train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.drop([training_data.columns[0]],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_data = pd.read_csv(\"Devnagari_Handwritten_Character_cv.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianNB:\n",
    "    \n",
    "    \"\"\"Instantiate a Gaussian Naive Bayes Object with the following parameters: \n",
    "        \n",
    "        features :               A dataframe consisting of continuous features, excluding labels\n",
    "        labels :                 A series consisting of binary labels\n",
    "        train_cv_test_split :    A tuple consisting of fraction for training, cross validation and testing data\n",
    "        apply_pca :              Boolean value specifying whether to apply PCA or not\n",
    "        n_principal_components : Number of Principal Components (Eigen vectors having non zero values to keep) \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,features,labels,train_cv_test_split,apply_pca,n_principal_components):\n",
    "        self.unique_labels = list(labels.unique())\n",
    "        self.labels = np.array(labels).reshape(labels.shape[0],1)\n",
    "        self.train_cv_test_split = train_cv_test_split\n",
    "        self.n_principal_components = n_principal_components\n",
    "        \n",
    "        if apply_pca == True:\n",
    "            self.X_new = self.apply_dim_reduction(features,self.n_principal_components)\n",
    "            \n",
    "            \n",
    "    def apply_dim_reduction(self,data,n_components):\n",
    "        X = np.array(data)\n",
    "        X_dash = X - np.mean(X,axis=0).reshape(-1,X.shape[1])\n",
    "        sigma_hat = (1/data.shape[0])*np.matmul(X_dash.T,X_dash)\n",
    "        sigma_hat_decompose = np.linalg.svd(sigma_hat)\n",
    "        Q = sigma_hat_decompose[0]\n",
    "        self.Q_tilda = Q[:,0:n_components]\n",
    "        X_new = np.matmul(X_dash,self.Q_tilda)\n",
    "        return X_new\n",
    "    \n",
    "    \n",
    "    def fit(self,data,alpha,gamma):\n",
    "        self.likelihood_params = dict()\n",
    "        sigma_hats = 0\n",
    "        \n",
    "        for label in self.unique_labels:\n",
    "            mu_hat = np.array(data[data['label'] == label].iloc[:,0:self.n_principal_components].mean())\n",
    "            sigma_hat = np.array(data[data['label'] == label].iloc[:,0:self.n_principal_components].cov())\n",
    "            sigma_hats = sigma_hats + 1359*sigma_hat\n",
    "            self.likelihood_params[label] = [mu_hat,sigma_hat]\n",
    "            \n",
    "        self.sigma_hat = sigma_hats/(data.shape[0] - len(self.unique_labels))\n",
    "        self.mean_variance = np.mean(np.diag(self.sigma_hat))\n",
    "            \n",
    "        for label in self.unique_labels:\n",
    "            self.likelihood_params[label][1] = alpha*self.likelihood_params[label][1] + (1-alpha)*self.sigma_hat\n",
    "            self.likelihood_params[label][1] = (1-gamma)*self.likelihood_params[label][1] + gamma*self.mean_variance*np.eye(self.n_principal_components,self.n_principal_components)\n",
    "        \n",
    "        \n",
    "    def evaluate(self,data):\n",
    "        inputs = np.array(data.iloc[:,0:self.n_principal_components])\n",
    "        posterior = list()\n",
    "        \n",
    "        for label in self.unique_labels:\n",
    "            posterior.append(s.multivariate_normal.pdf(inputs,self.likelihood_params[label][0],self.likelihood_params[label][1]).reshape(inputs.shape[0],1))\n",
    "        \n",
    "        posterior = np.concatenate(posterior,axis=1)\n",
    "        predicted_category = pd.Series(np.argmax(posterior,axis=1))\n",
    "        predicted_category.replace(to_replace=np.arange(0,len(self.unique_labels)),value=self.unique_labels,inplace=True)\n",
    "        predicted_results = np.array(predicted_category)\n",
    "        actual_results = np.array(data['label'])\n",
    "        acc = accuracy_score(y_true=actual_results,y_pred=predicted_results)\n",
    "        recall = recall_score(y_true=actual_results,y_pred=predicted_results,average='weighted')\n",
    "        precision = precision_score(y_true=actual_results,y_pred=predicted_results,average='weighted')\n",
    "        return {\"acc\":acc,\"recall\":recall,\"precision\":precision}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = dict()\n",
    "\n",
    "for n_components in [50,100,150,200,250,300,350,400,450,500,550,600,650,700]:\n",
    "    obj = GaussianNB(features=training_data.iloc[:,0:1024],labels=training_data.iloc[:,1024],train_cv_test_split=(0.7,0.2,0.1),\n",
    "                             apply_pca=True,n_principal_components=n_components)\n",
    "    X_train = pd.DataFrame(obj.X_new)\n",
    "    X_train['label'] = training_data['label']\n",
    "    obj.fit(X_train,0.5,0.3)\n",
    "    X_cv = np.matmul(np.array(cv_data.iloc[:,0:1024]),obj.Q_tilda)\n",
    "    X_cv = pd.DataFrame(data=X_cv)\n",
    "    X_cv['label'] = cv_data['label']\n",
    "    D[n_components] = obj.evaluate(X_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
